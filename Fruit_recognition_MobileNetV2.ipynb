{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Model MobileNETv2 --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import framework\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Conv2D, MaxPooling2D, Concatenate, Add, Activation, Multiply, Input, UpSampling2D, GlobalMaxPooling2D , Add, BatchNormalization, DepthwiseConv2D\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coumt the number of images in the training folder\n",
    "count = 0 \n",
    "dirs = os.listdir('Fruits Data 20/val/')\n",
    "for dir in dirs:\n",
    "    files = list(os.listdir('Fruits Data 20/val/'+dir))\n",
    "    print( dir +' Folder has '+ str(len(files)) + ' Images')\n",
    "    count = count + len(files)\n",
    "print( 'Images Folder has '+ str(count) + ' Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train = 'Fruits Data 20/train/'\n",
    "base_val = 'Fruits Data 20/val/'\n",
    "IMG_SIZE = 224  \n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_train(image, label):\n",
    "    # resize \n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    \n",
    "    # Augmentation\n",
    "    image = tf.image.random_brightness(image, 0.1)  \n",
    "    image = tf.image.random_flip_left_right(image)  \n",
    "    image = tf.image.random_flip_up_down(image)     \n",
    "    image = tf.image.rot90(image, k=tf.random.uniform([], 0, 4, dtype=tf.int32))  # Xoay ngẫu nhiên 0-270 độ\n",
    "    \n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))  \n",
    "    \n",
    "    image = preprocess_input(image)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    base_train,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical',\n",
    "    seed=42\n",
    ").shuffle(1000).map(preprocess_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    base_val,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical',\n",
    "    seed=42\n",
    ").map(lambda image, label: (preprocess_input(image), label),  # Chỉ chuẩn hóa [-1, 1]\n",
    "      num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show image after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow((images[i] + 1) / 2)\n",
    "        plt.title(labels[i])\n",
    "        plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Res-Inception Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_inception_module(inputs, filters_1x1, filters_3x3_reduce, filters_3x3,\n",
    "                        filters_5x5_reduce, filters_5x5, filters_pool):\n",
    "    # Nhánh 1: Convolution 1x1\n",
    "    branch_1x1 = Conv2D(filters_1x1, (1, 1), padding='same', activation='relu')(inputs) \n",
    "\n",
    "    # Nhánh 2: 1x1 giảm kênh + 3x3\n",
    "    branch_3x3 = Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu')(inputs) \n",
    "    branch_3x3 = Conv2D(filters_3x3, (3, 3), padding='same', activation='relu')(branch_3x3)\n",
    "\n",
    "    # Nhánh 3: 1x1 giảm kênh + 5x5\n",
    "    branch_5x5 = Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu')(inputs)\n",
    "    branch_5x5 = Conv2D(filters_5x5, (5, 5), padding='same', activation='relu')(branch_5x5)\n",
    "\n",
    "    # Nhánh 4: Pooling + 1x1\n",
    "    branch_pool = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    branch_pool = Conv2D(filters_pool, (1, 1), padding='same', activation='relu')(branch_pool)\n",
    "\n",
    "    # Nối các nhánh\n",
    "    inception_output = Concatenate(axis=-1)([branch_1x1, branch_3x3, branch_5x5, branch_pool])\n",
    "\n",
    "    # Residual connection\n",
    "    if inputs.shape[-1] != inception_output.shape[-1]:\n",
    "        shortcut = Conv2D(inception_output.shape[-1], (1, 1), padding='same')(inputs)\n",
    "    else:\n",
    "        shortcut = inputs\n",
    "\n",
    "    outputs = Add()([shortcut, inception_output])\n",
    "    outputs = Activation('relu')(outputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Multi-scale Attention (EMA) Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ema_module(inputs, filters):\n",
    "    # Multi-scale branches\n",
    "    conv_1x1 = Conv2D(filters // 2, (1, 1), padding='same', activation='relu')(inputs)\n",
    "    conv_3x3 = Conv2D(filters // 2, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    # Combined pooling: Global Average Pooling và Global Max Pooling\n",
    "    global_avg_pool = GlobalAveragePooling2D(keepdims=True)(inputs) # tính trung bình tất cả các pixel trong vùng \n",
    "    global_max_pool = GlobalMaxPooling2D(keepdims=True)(inputs) # giữ lại đặc trưng nổi bậc nhất, giảm overfitting < mất thông tin về vùng không có giá trị lớn \n",
    "\n",
    "    # combine pooling\n",
    "    global_pool = Add()([global_avg_pool, global_max_pool])  \n",
    "    global_pool = Conv2D(filters // 2, (1, 1), padding='same', activation='relu')(global_pool)\n",
    "    global_pool = UpSampling2D(size=(inputs.shape[1], inputs.shape[2]))(global_pool)\n",
    "\n",
    "    # Concatenate multi-scale features\n",
    "    multi_scale = Concatenate()([conv_1x1, conv_3x3, global_pool])\n",
    "\n",
    "    # Attention weights\n",
    "    attention = Conv2D(filters, (1, 1), padding='same')(multi_scale)\n",
    "    attention = Activation('sigmoid')(attention)\n",
    "\n",
    "    # Apply attention\n",
    "    outputs = Multiply()([inputs, attention])\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu6(x):\n",
    "    return Activation('relu6')(x) \n",
    "\n",
    "def bottleneck_block(inputs, filters, expansion, stride):\n",
    "    \"\"\"\n",
    "    inputs: tensor đầu vào\n",
    "    filters: số bộ lọc ở tầng pointwise cuối cùng\n",
    "    expansion: hệ số mở rộng số kênh trong bottleneck\n",
    "    stride: bước stride cho depthwise convolution\n",
    "    \"\"\"\n",
    "    in_channels = inputs.shape[-1] \n",
    "    \n",
    "    # 1. Expansion layer \n",
    "    x = Conv2D(filters * expansion, (1, 1), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = relu6(x)\n",
    "\n",
    "    # 2. Depthwise Convolution\n",
    "    x = DepthwiseConv2D((3, 3), strides=stride, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = relu6(x)\n",
    "\n",
    "    # 3. Projection layer \n",
    "    x = Conv2D(filters, (1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Skip connection \n",
    "    if stride == 1 and in_channels == filters:\n",
    "        x = Add()([inputs, x])\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve Model MobinetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MobileNetV2 with pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built Model MobileNetV2 with res_inception_module and ema_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng mô hình cải tiến\n",
    "inputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = base_model(inputs, training=False)  # MobileNetV2 backbone\n",
    "\n",
    "# Res-Inception Module \n",
    "x = res_inception_module(x, \n",
    "                         filters_1x1=64, \n",
    "                         filters_3x3_reduce=96, filters_3x3=128,\n",
    "                         filters_5x5_reduce=16, filters_5x5=32,\n",
    "                         filters_pool=32) \n",
    "\n",
    "# EMA Module \n",
    "x = ema_module(x, filters=256)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)  \n",
    "\n",
    "x = Dense(64, activation='relu6')(x) \n",
    "x = Dropout(0.2)(x) # Dropout 20% to prevent overfitting\n",
    "predictions = Dense(20, activation='softmax')(x) # 20 fruit classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built Model MobileNetV2 with bottleneck_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng mô hình cải tiến\n",
    "inputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = base_model(inputs, training=False)  # MobileNetV2 backbone\n",
    "\n",
    "x = bottleneck_block(x, filters=256, expansion=6, stride=1)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)  \n",
    "x = Dense(64, activation='relu6')(x) \n",
    "x = Dropout(0.2)(x) # Dropout 20% to prevent overfitting\n",
    "predictions = Dense(20, activation='softmax')(x)  # 20 fruit classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built Model MobileNetV2 with bottleneck_block and res_inception_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = base_model(inputs, training=False)  # MobileNetV2 backbone\n",
    "\n",
    "x = bottleneck_block(x, filters=256, expansion=6, stride=1)\n",
    "\n",
    "x = res_inception_module(x, \n",
    "                         filters_1x1=64, \n",
    "                         filters_3x3_reduce=96, filters_3x3=128,\n",
    "                         filters_5x5_reduce=16, filters_5x5=32,\n",
    "                         filters_pool=32)  \n",
    "\n",
    "x = GlobalAveragePooling2D()(x)  \n",
    "x = Dense(64, activation='relu6')(x) \n",
    "x = Dropout(0.2)(x) # Dropout 20% to prevent overfitting\n",
    "predictions = Dense(20, activation='softmax')(x)  #20 fruit classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complie mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs, outputs=predictions)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tóm tắt mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"MobileNet_Best_Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vẽ biểu đồ kết quả\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('MobileNetV2 - Fruits Classification')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# path\n",
    "model_path = 'path.h5'\n",
    "base_val = 'path_val'\n",
    "base_test = 'path_test'\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 1. load model\n",
    "try:\n",
    "    model = load_model(model_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {str(e)}\")\n",
    "    exit()\n",
    "\n",
    "# 2. size model\n",
    "file_size_bytes = os.path.getsize(model_path)\n",
    "file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "print(f\"Model Size: {file_size_mb:.2f} MB ({file_size_bytes} bytes)\")\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "# 4. Evaluating on Val Set\n",
    "print(\"\\nEvaluating on Val Set:\")\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    base_val,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "val_loss, val_accuracy = model.evaluate(val_generator)\n",
    "print(f\"Val Loss: {val_loss:.4f}\")\n",
    "print(f\"Val Accuracy: {val_accuracy:.4f} ({val_accuracy * 100:.2f}%)\")\n",
    "\n",
    "# 5. Evaluating on Test Set\n",
    "print(\"\\nEvaluating on Test Set:\")\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    base_test,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy * 100:.2f}%)\")\n",
    "\n",
    "# 6. F1-score\n",
    "test_generator.reset()  \n",
    "y_pred = model.predict(test_generator)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = test_generator.classes\n",
    "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
    "print(f\"F1-Score (Test): {f1:.4f}\")\n",
    "\n",
    "# 7. Parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
